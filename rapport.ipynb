{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Rapport \"Humeur d'une musique\"\n",
    "\n",
    "## Groupe:\n",
    "- Thomas Marchand LDD1 IM2\n",
    "- Frédéric Beceril LDD1 IM2\n",
    "\n",
    "## Jeux de données:\n",
    "Avant de nous risquer à quelque chose de plus dur, nous avons commencé par réaliser un classifier de référence avec le jeu de données `ZeroOne`, ce dernier est contenu dans le dossier ``./data/ZeroOne``. La feuille jupyter relative à ce dataset est contenue dans le dossier courant et se nomme ``analyse_de_donnees.ipynb``. Nous avons ensuite décidé de faire un classifier *original*. En effet nous n'avions joué qu'avec des images pendant les TD précédent et on s'est dit que ça pourrait être intéressant d'essayer avec de la musique ou du texte, mais nous avons gardé l'idée du texte pour le prochain projet :) !\n",
    "L'idée a donc été de créer un classifier capable de détecter si une musique est triste ou joyeuse. Nous avons pour cela téléchargé un total de 20 musiques (10 \"joyeuses\" et 10 \"tristes\") que nous avons placées dans le dossier ``./data/musics``.\n",
    "\n",
    "## Performances de référence\n",
    "Voir le code dans la partie ``Sandbox`` du fichier ``analyse_music.ipynb``\n",
    "Avant de réaliser notre implémentation nous avons passé beaucoup de temps (la majorité de notre temps en réalité) à chercher des features et à chercher leur pertinence. Grâce aux scatter_plots visibles dans la troisième partie du fichier ``analyse_music.ipynb`` nous avons pu choisir d'utiliser les feature tempo et spectral_centroid. Notre première implémentation utilisait un classifier FNN. Logiquement nous n'avions pas d'erreur avec l'ensemble d'entrainement. Avec l'ensemble de test par contre nous avions un taux d'erreurs vraiment trop élevé : 0.5.\n",
    "\n",
    "## Classifieurs alternatifs\n",
    "Voir le code dans la partie ``Sandbox`` du fichier ``analyse_music.ipynb``.\n",
    "Les performances du classifier FNN étaient vraiment décevantes. Nous avons aussi testé une classifieur OneR pour proposer quelque chose de plus simple et qui permettrait de séparer en utilisant uniquement la feature la plus importante : ça marchait plutôt bien (0.1 comme taux d'erreur sur l'ensemble de test) et ça avait le mérite d'être simple. Il était quand même dommage d'avoir 0.2 comme taux d'erreur sur l'ensemble d'entrainement. Nous avons donc essayé d'implémenter un KNN et avons testé différent paramètres. Jusqu'à k=3 les performances augmentaient, après cela elles ne faisaient que décroître. Le résultat avec k=3 était satisfaisant : 0.1 comme taux d'erreur sur l'ensemble d'entrainement et celui de test. \n",
    "\n",
    "## Conclusion\n",
    "La méthode la plus performante fut de réaliser un KNeighborsClassifier avec k=3 en s'appuyant sur le tempo et la spectral centroid (une mesure relative au timbre du son). Notre jeu de test n'était ni trop facile ni trop difficile à classer : nous sommes plutôt satisfaits du résultat. Je pense que c'est le cas car malgré des features un peu compliquées à utiliser, nous avons choisi des musiques européennes avec des différences plutôt marquées.\n",
    "Nous nous sommes partagés la réalisation des graphiques et avons analysé les résultats ensembles. Frédéric a pu apporter son expérience en musique et j'ai (Thomas) implémenté les classifieurs. La plus grosse difficulté que nous ayons rencontré fut de trouver les bonnes features : on ne s'attendait pas à ce résultat. Monsieur Teo Sanchez nous a donné des pistes pour améliorer le classifier pendant le dernier TD mais malheureusement cela n'a pas donné de meilleurs résultats.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}