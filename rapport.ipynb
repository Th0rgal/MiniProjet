{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# \"Mood of a Music\" Report\n",
    "\n",
    "## Group:\n",
    "- Thomas Marchand LDD1 IM2\n",
    "- Frédéric Beceril LDD1 IM2\n",
    "\n",
    "## Data sets:\n",
    "Before venturing into something harder, we started by making a reference classifier with the dataset `ZeroOne`, the latter is contained in the folder ``./data/ZeroOne``. The jupyter sheet related to this dataset is contained in the current folder and is named ``data_analysis.ipynb``. We then decided to create an *original* classifier. Indeed we had only played with images during the previous tutorials and we thought that it could be interesting to try with music or text (we kept the text idea for the next project) :) !\n",
    "The idea was to create a classifier able to detect if a music is sad or happy. We downloaded a total of 20 musics (10 \"happy\" and 10 \"sad\") that we put in the folder ``./data/musics``.\n",
    "\n",
    "## Reference performance\n",
    "See the code in the Sandbox part of the file ``analyse_music.ipynb``.\n",
    "Before making our implementation we spent a lot of time (most of our time actually) looking for features and searching for their relevance. Thanks to the scatter_plots visible in the third part of the ``analyse_music.ipynb`` file we could choose to use the tempo and spectral_centroid features. Our first implementation used an FNN classifier. Logically we had no error with the training set. With the test set however we had a really high error rate: 0.5.\n",
    "\n",
    "## Alternative classifiers\n",
    "See the code in the Sandbox part of the file ``analyse_music.ipynb``.\n",
    "The performance of the FNN classifier was really disappointing. We also tested a OneR classifier to propose something simpler and that would allow to separate using only the most important feature: it worked quite well (0.1 as error rate on the test set) and it had the merit to be simple. It was still a pity to have 0.2 as the error rate on the training set. So we tried to implement a KNN and tested different parameters. Up to k=3 the performance increased, after that it only decreased. The result with k=3 was satisfactory: 0.1 as the error rate on the training set and the test set. \n",
    "\n",
    "## Alternative features\n",
    "See the code in the file ``analyse_music_experiment.ipynb``.\n",
    "After extracting all the features avaible on Librosa we found that the spectral_centroid was the most efficient one.\n",
    "The performance of the scale extraction was disappointing. Even after a filter on the music the correlation wasn't greater than 0.2, and the error rate combine with the spectral_centroid was higher simulat than the error rate of the spectral_centroid and the tempo combine. But it was less efficient, and slowed greatly the extraction of the features.\n",
    "\n",
    "## Conclusion\n",
    "The most efficient method was to perform a KNeighborsClassifier with k=3 based on the tempo and the spectral centroid (a measure of the timbre of the sound). Our test set was neither too easy nor too difficult to classify: we are rather satisfied with the result. I think that's the case because despite some complicated features to use, we chose European music with rather marked differences.\n",
    "We shared the realization of the graphics and analyzed the results together. Frédéric could bring his experience in music and I (Thomas) implemented the classifiers. The biggest difficulty we had was to find the right features: we didn't expect this result. Mr. Teo Sanchez gave us some hints to improve the classifier during the last tutorial but unfortunately it didn't give better results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}